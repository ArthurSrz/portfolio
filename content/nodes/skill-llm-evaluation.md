---
title: "LLM Evaluation & Experimentation"
type: "nodes"
id: "skill-llm-evaluation"
shape: "pentagon"
parent: "skills"
subtitle: "Observability & Metrics"
connectionType: "solid"
weight: 10
draft: false
---

Expertise in evaluating and experimenting with Large Language Models in production environments:

- **Tracing & Observability**: Chain-of-thought logging, latency tracking, token usage monitoring
- **Evaluation Frameworks**: Building evaluation datasets, defining metrics, automated scoring
- **Experiment Tracking**: A/B testing prompts, comparing model versions, tracking regressions
- **Alignment Workflows**: Human feedback collection, annotation interfaces, RLHF pipelines

Tools: Opik, LangSmith, Weights & Biases, custom evaluation harnesses
