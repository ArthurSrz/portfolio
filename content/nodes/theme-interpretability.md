---
title: "Interpretability"
type: "nodes"
id: "theme-interpretability"
shape: "octagon"
parent: "research-themes"
subtitle: "Research Theme"
connectionLabel: ""
connectionType: "solid"
weight: 10
draft: false
---

# Interpretability Research Theme

Making complex systems—from LLMs to data ecosystems—understandable and navigable.

## Core Premise

Complex systems don't have to be black boxes. Through careful design, we can create interfaces that make their internal mechanisms visible and actionable.

## Theoretical Foundations

- **Attribution Theory**: Understanding which inputs influence which outputs
- **Design Science**: Creating artifacts that enhance interpretability
- **Attention Economics**: Designing for limited cognitive resources

## Related Work

- [Inside LLMs' Mind](/nodes/inside-llms-mind) - LLM decision visibility
- [Borges Graph](/nodes/borges-graph) - Knowledge navigation
- Anthropic's attribution graphs (inspiration)
