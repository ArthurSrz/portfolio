---
title: "LLM Interpretability"
type: "nodes"
id: "skill-llm-interpretability"
shape: "triangle"
parent: "skills"
subtitle: "advanced | technical"
connectionLabel: ""
connectionType: "solid"
weight: 10
draft: false
---

# LLM Interpretability

Advanced capability in making Large Language Model decision-making visible and understandable.

## Philosophy

LLMs are not necessarily black boxes. They can be understood as "usable and entire lines of reasoning" accessible to users who engage creatively with them.

## Techniques

- **Chain Routing Visualization**: Making visible which reasoning pathway the system selects
- **Tool Use Transparency**: Showing when LLMs use external tools (search, calculation)
- **Attribution Analysis**: Understanding which inputs influence which outputs

## Related Projects

- [Inside LLMs' Mind](/nodes/inside-llms-mind) - Application making LLM decisions visible
- [Outside LLMs Mind](/nodes/outside-llms-mind) - External behavioral analysis
- [Oracles ou Romanciers](/nodes/oracles-ou-romanciers) - Research on LLM prediction vs simulation
